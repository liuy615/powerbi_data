import pandas as pd
import numpy as np
import os
import logging
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Dict, Tuple, Any
from logging.handlers import RotatingFileHandler
from pymongo import MongoClient
import datetime

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, message="Data Validation extension is not supported and will be removed")
warnings.filterwarnings("ignore", category=FutureWarning, message=".*DataFrame concatenation with empty or all-NA entries.*")
pd.set_option('display.max_columns', 100)

"""æ•°æ®å¤„ç†åŸºç±»ï¼Œæä¾›ç»Ÿä¸€çš„æ—¥å¿—å’Œå¼‚å¸¸å¤„ç†"""
class DataProcessorBase:


    def __init__(self, processor_name: str, base_data_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"):
        self.processor_name = processor_name
        self.base_data_dir = base_data_dir
        self.logger = self._setup_logger()

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_dir = os.path.join(self.base_data_dir, "powerbi_data", "data", "ç§æœ‰äº‘æ—¥å¿—", "logs")
        os.makedirs(log_dir, exist_ok=True)

        log_filename = f"{self.processor_name}_{datetime.datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = os.path.join(log_dir, log_filename)

        logger = logging.getLogger(self.processor_name)
        logger.setLevel(logging.INFO)
        logger.propagate = False

        # é¿å…é‡å¤æ·»åŠ handler
        if not logger.handlers:
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)

            # æ–‡ä»¶å¤„ç†å™¨
            file_handler = RotatingFileHandler(
                log_filepath,
                maxBytes=10 * 1024 * 1024,
                backupCount=5,
                encoding="utf-8"
            )
            file_handler.setLevel(logging.INFO)

            # æ ¼å¼åŒ–å™¨
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                datefmt="%Y-%m-%d %H:%M:%S"
            )

            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)

            logger.addHandler(console_handler)
            logger.addHandler(file_handler)

        return logger

    def safe_execute(self, func, *args, **kwargs):
        """å®‰å…¨æ‰§è¡Œæ–¹æ³•ï¼Œæ•è·å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—"""
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡Œ: {func.__name__}")
            result = func(*args, **kwargs)
            self.logger.info(f"å®Œæˆæ‰§è¡Œ: {func.__name__}")
            return result
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå¤±è´¥ {func.__name__}: {str(e)}", exc_info=True)
            return None

"""MongoDBè¿æ¥é…ç½®ç±»"""
class MongoDBConfig:
    def __init__(self, host='192.168.1.7', port=27017,username='xg_wd', password='H91NgHzkvRiKygTe4X4ASw',auth_source='xg', database='xg_JiaTao'):
        self.host = host
        self.port = port
        self.username = username
        self.password = password
        self.auth_source = auth_source
        self.database_name = database

    def get_connection_string(self):
        """æ„å»ºè¿æ¥å­—ç¬¦ä¸²"""
        return f'mongodb://{self.username}:{self.password}@{self.host}:{self.port}/{self.database_name}?authSource={self.auth_source}&authMechanism=SCRAM-SHA-256'

    def get_database_name(self):
        return self.database_name

"""MongoDBå®¢æˆ·ç«¯æ“ä½œç±»"""
class MongoDBClient:
    def __init__(self, config):
        self.config = config
        self.client = None
        self.db = None
        self.connected = False

    def connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.client = MongoClient(self.config.get_connection_string())
            self.db = self.client[self.config.get_database_name()]

            # æµ‹è¯•è¿æ¥
            self.client.admin.command('ping')
            self.connected = True
            print("æˆåŠŸè¿æ¥åˆ°MongoDB!")
            return True

        except Exception as e:
            print(f"è¿æ¥å¤±è´¥: {e}")
            self.connected = False
            return False

    def disconnect(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.client:
            self.client.close()
            self.connected = False
            print("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def get_collection_count(self, collection_name):
        """è·å–æŒ‡å®šé›†åˆçš„æ–‡æ¡£æ•°é‡"""
        if not self.connected:
            print("æœªå»ºç«‹æ•°æ®åº“è¿æ¥")
            return 0

        try:
            collection = self.db[collection_name]
            return collection.count_documents({})
        except Exception as e:
            print(f"è·å–æ–‡æ¡£æ•°é‡å¤±è´¥: {e}")
            return 0

    def query_data_with_projection(self, collection_name, desired_fields, limit=None, query_filter=None):
        """æŸ¥è¯¢æŒ‡å®šé›†åˆä¸­æŒ‡å®šå­—æ®µçš„æ•°æ®"""
        if not self.connected:
            print("æœªå»ºç«‹æ•°æ®åº“è¿æ¥")
            return None

        try:
            # è·å–æŒ‡å®šé›†åˆ
            collection = self.db[collection_name]

            # åŠ¨æ€æ„å»ºæŠ•å½±
            projection = {field: 1 for field in desired_fields}
            projection["_id"] = 0  # ä¸è¿”å›_id

            # è®¾ç½®æŸ¥è¯¢è¿‡æ»¤å™¨ï¼Œé»˜è®¤ä¸ºç©º
            if query_filter is None:
                query_filter = {}

            # æŸ¥è¯¢æ•°æ®
            query = collection.find(query_filter, projection)
            if limit:
                query = query.limit(limit)

            # è½¬æ¢ä¸ºåˆ—è¡¨å’ŒDataFrame
            data_list = list(query)
            df = pd.DataFrame(data_list)

            return df

        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def query_all_data(self, collection_name, limit=None, query_filter=None, exclude_id=True):
        """æŸ¥è¯¢æŒ‡å®šé›†åˆä¸­çš„æ‰€æœ‰æ•°æ®"""
        if not self.connected:
            print("æœªå»ºç«‹æ•°æ®åº“è¿æ¥")
            return None

        try:
            # è·å–æŒ‡å®šé›†åˆ
            collection = self.db[collection_name]

            # è®¾ç½®æŠ•å½±ï¼Œé»˜è®¤ä¸è¿”å›_idå­—æ®µ
            projection = {"_id": 0} if exclude_id else {}

            # è®¾ç½®æŸ¥è¯¢è¿‡æ»¤å™¨ï¼Œé»˜è®¤ä¸ºç©º
            if query_filter is None:
                query_filter = {}

            # æŸ¥è¯¢æ•°æ®
            query = collection.find(query_filter, projection)
            if limit:
                query = query.limit(limit)

            # è½¬æ¢ä¸ºåˆ—è¡¨å’ŒDataFrame
            data_list = list(query)
            df = pd.DataFrame(data_list)

            return df

        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def list_collections(self):
        """åˆ—å‡ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰é›†åˆ"""
        if not self.connected:
            print("æœªå»ºç«‹æ•°æ®åº“è¿æ¥")
            return []

        try:
            collections = self.db.list_collection_names()
            print("æ•°æ®åº“ä¸­çš„é›†åˆåˆ—è¡¨:")
            for i, collection in enumerate(collections, 1):
                print(f"{i}. {collection}")
            return collections
        except Exception as e:
            print(f"è·å–é›†åˆåˆ—è¡¨å¤±è´¥: {e}")
            return []

    def get_collection_fields(self, collection_name, sample_size=5):
        """è·å–æŒ‡å®šé›†åˆçš„å­—æ®µä¿¡æ¯"""
        if not self.connected:
            print("æœªå»ºç«‹æ•°æ®åº“è¿æ¥")
            return []

        try:
            collection = self.db[collection_name]
            sample_doc = collection.find_one()
            if sample_doc:
                fields = list(sample_doc.keys())
                print(f"é›†åˆ '{collection_name}' çš„å­—æ®µ:")
                for field in fields:
                    print(f"- {field}")
                return fields
            else:
                print(f"é›†åˆ '{collection_name}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
                return []
        except Exception as e:
            print(f"è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {e}")
            return []

"""è¥é”€æŠ•æ”¾è´¹ç”¨æ•°æ®å¤„ç†å™¨"""
class YingxiaoMoneyProcessor(DataProcessorBase):


    def __init__(self, base_data_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"):
        super().__init__("è¥é”€æŠ•æ”¾è´¹ç”¨å¤„ç†å™¨", base_data_dir)

        # è·¯å¾„é…ç½®
        self.input_dir = os.path.join(base_data_dir, "ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°", "æŠ•æ”¾å¸‚åœºè´¹ç”¨")
        self.output_dir = os.path.join(base_data_dir, "dashboard")
        self.output_file = os.path.join(self.output_dir, "æŠ•æ”¾è´¹ç”¨.csv")

        # ä¸šåŠ¡é…ç½®
        self.target_sheets = ["2024å¹´", "2025å¹´", "2026å¹´"]
        self.required_columns = ["å¹´ä»½", "æœˆä»½", "å½’å±é—¨åº—", "é¡¹ç›®å¤§ç±»", "é¡¹ç›®åˆ†ç±»", "å…·ä½“é¡¹ç›®","è´¹ç”¨é‡‘é¢", "æ ¸é”€å‘ç¥¨é‡‘é¢", "æ ¸é”€å‘ç¥¨ç¨é‡‘", "è´¹ç”¨åˆè®¡", "å¤‡æ³¨", "From"]
        self.store_map = {"æ–‡æ™¯åˆæ²»": "ä¸Šå…ƒç››ä¸–", "ç‹æœç½‘-ç›´æ’­åŸºåœ°":"ç›´æ’­åŸºåœ°", "æ°¸ä¹ç››ä¸–":"æ´ªæ­¦ç››ä¸–"}

        self._init_check()

    def _init_check(self):
        """ç¯å¢ƒæ£€æŸ¥"""
        if not os.path.exists(self.input_dir):
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼š{self.input_dir}")
        os.makedirs(self.output_dir, exist_ok=True)
        self.logger.info(f"åˆå§‹åŒ–å®Œæˆ - è¾“å…¥ç›®å½•: {self.input_dir}, è¾“å‡ºç›®å½•: {self.output_dir}")

    def _get_excel_files(self) -> List[str]:
        """è·å–æ‰€æœ‰Excelæ–‡ä»¶è·¯å¾„"""
        excel_files = [
            os.path.join(self.input_dir, f)
            for f in os.listdir(self.input_dir)
            if f.endswith(".xlsx")
        ]
        self.logger.info(f"æ‰¾åˆ° {len(excel_files)} ä¸ªExcelæ–‡ä»¶")
        return excel_files

    def _process_single_file(self, file_path: str) -> Optional[pd.DataFrame]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_name = os.path.basename(file_path)
        try:
            dfs = []
            with pd.ExcelFile(file_path) as xls:
                for sheet in self.target_sheets:
                    if sheet in xls.sheet_names:
                        df = pd.read_excel(xls, sheet)
                        df["From"] = file_name.split('.')[0]
                        if not df.empty:
                            dfs.append(df)

            if dfs:
                df_comb = pd.concat(dfs, axis=0, ignore_index=True)
                self.logger.info(f"å¤„ç†æ–‡ä»¶ {file_name} æˆåŠŸ - {len(df_comb)}è¡Œ")
                return df_comb
            else:
                self.logger.warning(f"æ–‡ä»¶ {file_name} æ— ç›®æ ‡å·¥ä½œè¡¨æ•°æ®")
                return None
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_name} å¤±è´¥: {str(e)}")
            return None

    def _process_all_files(self, excel_files: List[str]) -> pd.DataFrame:
        """å¤šçº¿ç¨‹å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç† {len(excel_files)} ä¸ªæ–‡ä»¶")
        combined = []

        with ThreadPoolExecutor() as executor:
            results = executor.map(self._process_single_file, excel_files)
            for res in results:
                if res is not None:
                    combined.append(res)

        if not combined:
            raise ValueError("æ— æœ‰æ•ˆæ•°æ®å¯åˆå¹¶")

        df_all = pd.concat(combined, axis=0, ignore_index=True)
        self.logger.info(f"æ–‡ä»¶åˆå¹¶å®Œæˆ - æ€»è®¡ {len(df_all)} è¡Œæ•°æ®")
        return df_all

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        exist_cols = [c for c in self.required_columns if c in df.columns]
        df_clean = df[exist_cols].copy()

        if "è´¹ç”¨åˆè®¡" in df_clean.columns and "è´¹ç”¨é‡‘é¢" in df_clean.columns:
            df_clean["è´¹ç”¨åˆè®¡"] = df_clean["è´¹ç”¨åˆè®¡"].fillna(df_clean["è´¹ç”¨é‡‘é¢"])
            df_clean["è´¹ç”¨åˆè®¡"] = pd.to_numeric(df_clean["è´¹ç”¨åˆè®¡"], errors="coerce").fillna(0)

        self.logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ - {len(df_clean)}è¡Œ, {len(exist_cols)}åˆ—")
        return df_clean

    def _replace_store(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ›¿æ¢é—¨åº—åç§°"""
        if "å½’å±é—¨åº—" in df.columns:
            df["å½’å±é—¨åº—"] = df["å½’å±é—¨åº—"].replace(self.store_map)
            self.logger.info("é—¨åº—åç§°æ›¿æ¢å®Œæˆ")
        return df

    def _save_result(self, df: pd.DataFrame) -> None:
        """ä¿å­˜ç»“æœ"""
        df.to_csv(self.output_file, index=False, encoding="utf-8-sig")
        self.logger.info(f"ç»“æœä¿å­˜æˆåŠŸ: {self.output_file} - {len(df)}è¡Œ")

    def run(self) -> bool:
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹è¥é”€æŠ•æ”¾è´¹ç”¨å¤„ç†æµç¨‹")
        self.logger.info("=" * 60)

        try:
            excel_files = self.safe_execute(self._get_excel_files)
            if not excel_files:
                self.logger.error("æœªæ‰¾åˆ°Excelæ–‡ä»¶")
                return False

            df_all = self.safe_execute(self._process_all_files, excel_files)
            if df_all is None:
                return False

            df_clean = self.safe_execute(self._clean_data, df_all)
            df_final = self.safe_execute(self._replace_store, df_clean)
            self.safe_execute(self._save_result, df_final)

            self.logger.info("è¥é”€æŠ•æ”¾è´¹ç”¨å¤„ç†å®Œæˆ!")
            return True

        except Exception as e:
            self.logger.error(f"è¥é”€æŠ•æ”¾è´¹ç”¨å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            return False


"""ç‰¹æ®Šäº‹é¡¹æ”¶å…¥æ•°æ®å¤„ç†å™¨"""
class SpecialIncomeProcessor(DataProcessorBase):


    def __init__(self, base_data_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"):
        super().__init__("ç‰¹æ®Šäº‹é¡¹æ”¶å…¥å¤„ç†å™¨", base_data_dir)

        self.directories = [os.path.join(base_data_dir, "ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°", "ç‰¹æ®Šäº‹é¡¹æ”¶å…¥")]
        self.output_path = os.path.join(base_data_dir, "dashboard", "ç‰¹æ®Šè´¹ç”¨æ”¶å…¥.csv")
        self.target_sheet = 'ç™»è®°è¡¨'
        self.required_columns = [
            'ä¸šåŠ¡æ—¶é—´', 'å½’å±é—¨åº—', 'è½¦æ¶å·', 'å®¢æˆ·åç§°', 'äº‹é¡¹åç§°',
            'æ”¶ä»˜ç±»å‹', 'é‡‘é¢', 'å¤‡æ³¨', 'From'
        ]

        self.df_result = pd.DataFrame()

    def _get_all_file_paths(self):
        """è·å–æ‰€æœ‰Excelæ–‡ä»¶è·¯å¾„"""
        file_paths = []
        for folder_path in self.directories:
            if os.path.exists(folder_path):
                for file_name in os.listdir(folder_path):
                    if file_name.endswith('.xlsx'):
                        file_path = os.path.join(folder_path, file_name)
                        file_paths.append(file_path)
        self.logger.info(f"æ‰¾åˆ° {len(file_paths)} ä¸ªExcelæ–‡ä»¶")
        return file_paths

    def _process_single_file(self, file_path):
        """å¤„ç†å•ä¸ªExcelæ–‡ä»¶"""
        try:
            with pd.ExcelFile(file_path) as xls:
                if self.target_sheet not in xls.sheet_names:
                    self.logger.warning(f"æ–‡ä»¶ {os.path.basename(file_path)} æ— ç›®æ ‡å·¥ä½œè¡¨")
                    return None

                data = pd.read_excel(xls, sheet_name=self.target_sheet)
                data['From'] = os.path.basename(file_path).split('.')[0]

                self.logger.info(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æˆåŠŸ - {len(data)}è¡Œ")
                return data

        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} å¤±è´¥: {str(e)}")
            return None

    def _process_data_quality(self):
        """æ•°æ®è´¨é‡å¤„ç†"""
        existing_columns = [col for col in self.required_columns if col in self.df_result.columns]
        self.df_result = self.df_result[existing_columns]

        if 'é‡‘é¢' in self.df_result.columns:
            self.df_result['é‡‘é¢'] = pd.to_numeric(self.df_result['é‡‘é¢'], errors='coerce').fillna(0)

        if 'å½’å±é—¨åº—' in self.df_result.columns:
            self.df_result['å½’å±é—¨åº—'] = self.df_result['å½’å±é—¨åº—'].replace('æ–‡æ™¯åˆæ²»', 'ä¸Šå…ƒç››ä¸–')
            self.df_result['å½’å±é—¨åº—'] = self.df_result['å½’å±é—¨åº—'].replace("ç‹æœç½‘-ç›´æ’­åŸºåœ°", "ç›´æ’­åŸºåœ°")
            self.df_result['å½’å±é—¨åº—'] = self.df_result['å½’å±é—¨åº—'].replace("æ°¸ä¹ç››ä¸–", "æ´ªæ­¦ç››ä¸–")

        self.logger.info(f"æ•°æ®è´¨é‡å¤„ç†å®Œæˆ - ä¿ç•™{len(existing_columns)}åˆ—")

    def load_data(self):
        """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®"""
        file_paths = self._get_all_file_paths()
        if not file_paths:
            self.logger.error("æœªæ‰¾åˆ°Excelæ–‡ä»¶")
            return False

        combined_data = []
        with ThreadPoolExecutor() as executor:
            results = executor.map(self._process_single_file, file_paths)
            for result in results:
                if result is not None and not result.empty:
                    combined_data.append(result)

        if combined_data:
            self.df_result = pd.concat(combined_data, ignore_index=True)
            self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ - {len(self.df_result)} è¡Œ")
            return True
        else:
            self.logger.error("æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
            return False

    def save_data(self):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        if self.df_result.empty:
            self.logger.error("æ— æ•°æ®å¯ä¿å­˜")
            return False

        try:
            os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
            self.df_result.to_csv(self.output_path, index=False)
            self.logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {self.output_path}")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
            return False

    def run(self) -> bool:
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹ç‰¹æ®Šäº‹é¡¹æ”¶å…¥æ•°æ®å¤„ç†æµç¨‹")
        self.logger.info("=" * 60)

        try:
            if not self.safe_execute(self.load_data):
                return False

            self.safe_execute(self._process_data_quality)

            if not self.safe_execute(self.save_data):
                return False

            self.logger.info("ç‰¹æ®Šäº‹é¡¹æ”¶å…¥æ•°æ®å¤„ç†å®Œæˆ!")
            return True

        except Exception as e:
            self.logger.error(f"ç‰¹æ®Šäº‹é¡¹æ”¶å…¥æ•°æ®å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            return False


"""æ•°æ®åˆå¹¶å¤„ç†å™¨"""
class DataMerger(DataProcessorBase):


    def __init__(self, base_data_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"):
        super().__init__("æ•°æ®åˆå¹¶å¤„ç†å™¨", base_data_dir)

        self.base_output_dir = os.path.join(base_data_dir, "dashboard")
        os.makedirs(self.base_output_dir, exist_ok=True)

        self.file_configs = self._init_file_configs()
        self.tasks: List[Dict[str, Any]] = [
            {
                "file_list_key": "sales_files",
                "sheet_name": "é”€é‡ç›®æ ‡",
                "process_type": "default",
                "output_filename": "merged_sales_data.xlsx",
                "replace_col": "å…¬å¸åç§°"
            },
            {
                "file_list_key": "quality_files",
                "sheet_name": "æœåŠ¡å“è´¨",
                "process_type": "filter_unnamed",
                "output_filename": "merged_quality_data.xlsx",
                "replace_col": "æœˆä»½"
            },
            {
                "file_list_key": "quality_files1",
                "sheet_name": "NPS",
                "process_type": "default",
                "output_filename": "merged_nps_data.xlsx",
                "replace_col": "å…¬å¸åç§°"
            },
            {
                "file_list_key": "quality_files1",
                "sheet_name": "æè½¦",
                "process_type": "default",
                "output_filename": "merged_tiche_data.xlsx",
                "replace_col": "å…¬å¸åç§°"
            },
            {
                "file_list_key": "wes_files",
                "sheet_name": "ä¸¤ç½‘",
                "process_type": "filter_score",
                "output_filename": "merged_wes_data.xlsx",
                "replace_col": "å…¬å¸åç§°"
            }
        ]

    def _init_file_configs(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ‰€æœ‰è¾“å…¥æ–‡ä»¶è·¯å¾„é…ç½®"""
        return {
            "sales_files": [
                r'E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2025å¹´é”€é‡æ±‡æ€»è¡¨.xlsx',
                r"E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2024å¹´é”€é‡æ±‡æ€»è¡¨.xlsx",
            ],
            "quality_files": [
                r'E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2025å¹´æ•°æ®æ±‡æ€»è¡¨.xlsx',
                r'E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2024å¹´æ•°æ®æ±‡æ€»è¡¨.xlsx'
            ],
            "quality_files1": [
                r'E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2025å¹´æ•°æ®æ±‡æ€»è¡¨.xlsx',
                r'E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2024å¹´æ•°æ®æ±‡æ€»è¡¨.xlsx'
            ],
            "wes_files": [
                r"E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ”¶é›†æ–‡ä»¶\2025å¹´WESè¿”åˆ©æ±‡æ€»è¡¨.xlsx"
            ]
        }

    def _process_single_file(self, file_path: str, sheet_name: str) -> Optional[pd.DataFrame]:
        """å¤„ç†å•ä¸ªExcelæ–‡ä»¶çš„æŒ‡å®šå·¥ä½œè¡¨"""
        if not os.path.exists(file_path):
            self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None

        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            unpivot_df = pd.melt(df, id_vars=df.columns[0:2], var_name='å±æ€§', value_name='å€¼')
            self.logger.info(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æˆåŠŸ - {len(unpivot_df)}è¡Œ")
            return unpivot_df
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            return None

    def _process_file_list(self, file_list: List[str], sheet_name: str, process_type: str) -> Optional[pd.DataFrame]:
        """å¤„ç†æ–‡ä»¶åˆ—è¡¨"""
        dfs = []
        for file_path in file_list:
            df = self._process_single_file(file_path, sheet_name)
            if df is not None:
                dfs.append(df)

        if not dfs:
            self.logger.warning(f"æœªæ‰¾åˆ° {sheet_name} å·¥ä½œè¡¨çš„æœ‰æ•ˆæ•°æ®")
            return None

        merged_df = pd.concat(dfs, ignore_index=True)

        if process_type == "filter_unnamed":
            merged_df = merged_df[~merged_df['å±æ€§'].str.contains("Unnamed", na=False)]
        elif process_type == "filter_score":
            merged_df = merged_df[
                (merged_df['å±æ€§'].str.contains("å¾—åˆ†", na=False)) &
                (merged_df['å…¬å¸åç§°'].notna())
                ]

        self.logger.info(f"æ–‡ä»¶åˆ—è¡¨å¤„ç†å®Œæˆ - {sheet_name}: {len(merged_df)}è¡Œ")
        return merged_df

    def _replace_company_name(self, df: pd.DataFrame, col_name: str) -> pd.DataFrame:
        """æ›¿æ¢å…¬å¸åç§°"""
        if col_name in df.columns:
            df[col_name] = df[col_name].replace('æ–‡æ™¯åˆæ²»', 'ä¸Šå…ƒç››ä¸–')
            df[col_name] = df[col_name].replace("ç‹æœç½‘-ç›´æ’­åŸºåœ°", "ç›´æ’­åŸºåœ°")
            df[col_name] = df[col_name].replace("æ°¸ä¹ç››ä¸–", "æ´ªæ­¦ç››ä¸–")
            self.logger.info("å…¬å¸åç§°æ›¿æ¢å®Œæˆ")
        else:
            self.logger.warning(f"æ›¿æ¢åˆ— '{col_name}' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")
        return df

    def _save_result(self, df: pd.DataFrame, output_filename: str):
        """ä¿å­˜å¤„ç†ç»“æœ"""
        output_path = os.path.join(self.base_output_dir, output_filename)
        try:
            df.to_excel(output_path, index=False)
            self.logger.info(f"ç»“æœä¿å­˜æˆåŠŸ: {output_path}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶ {output_filename} å¤±è´¥: {str(e)}")

    def run(self) -> bool:
        """æ‰§è¡Œæ‰€æœ‰æ•°æ®å¤„ç†ä»»åŠ¡"""
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹æ•°æ®åˆå¹¶å¤„ç†æµç¨‹")
        self.logger.info("=" * 60)

        try:
            success_count = 0
            total_count = len(self.tasks)

            for task in self.tasks:
                self.logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: {task['sheet_name']} -> {task['output_filename']}")

                file_list = self.file_configs.get(task['file_list_key'], [])
                if not file_list:
                    self.logger.warning(f"æœªæ‰¾åˆ°æ–‡ä»¶åˆ—è¡¨ {task['file_list_key']}ï¼Œè·³è¿‡è¯¥ä»»åŠ¡")
                    continue

                merged_df = self._process_file_list(
                    file_list=file_list,
                    sheet_name=task['sheet_name'],
                    process_type=task['process_type']
                )

                if merged_df is None or merged_df.empty:
                    self.logger.warning(f"{task['sheet_name']} å¤„ç†åæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                    continue

                merged_df = self._replace_company_name(merged_df, task['replace_col'])
                self._save_result(merged_df, task['output_filename'])
                success_count += 1

            self.logger.info(f"æ•°æ®åˆå¹¶å¤„ç†å®Œæˆ! æˆåŠŸ{success_count}/{total_count}ä¸ªä»»åŠ¡")
            return success_count > 0

        except Exception as e:
            self.logger.error(f"æ•°æ®åˆå¹¶å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            return False


"""ä¸»æ•°æ®å¤„ç†å™¨ - ç»Ÿä¸€è°ƒåº¦æ‰€æœ‰æ•°æ®å¤„ç†ä»»åŠ¡"""
class MainDataProcessor:


    def __init__(self, base_data_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"):
        self.base_data_dir = base_data_dir
        self.logger = self._setup_main_logger()
        self.processors = []

    def _setup_main_logger(self) -> logging.Logger:
        """è®¾ç½®ä¸»ç¨‹åºæ—¥å¿—é…ç½®"""
        log_dir = os.path.join(self.base_data_dir, "powerbi_data", "data", "ç§æœ‰äº‘æ—¥å¿—", "logs")
        os.makedirs(log_dir, exist_ok=True)

        log_filename = f"main_processor_{datetime.datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = os.path.join(log_dir, log_filename)

        logger = logging.getLogger("MainProcessor")
        logger.setLevel(logging.INFO)
        logger.propagate = False

        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)

            file_handler = RotatingFileHandler(
                log_filepath,
                maxBytes=10 * 1024 * 1024,
                backupCount=5,
                encoding="utf-8"
            )
            file_handler.setLevel(logging.INFO)

            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                datefmt="%Y-%m-%d %H:%M:%S"
            )

            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)

            logger.addHandler(console_handler)
            logger.addHandler(file_handler)

        return logger

    def run_all_processors(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®å¤„ç†å™¨"""
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹æ‰§è¡Œæ‰€æœ‰æ•°æ®å¤„ç†ä»»åŠ¡")
        self.logger.info("=" * 80)

        # å®šä¹‰å¤„ç†å™¨æ‰§è¡Œé¡ºåº
        processor_classes = [
            ("è¥é”€æŠ•æ”¾è´¹ç”¨å¤„ç†å™¨", YingxiaoMoneyProcessor),
            ("ç‰¹æ®Šäº‹é¡¹æ”¶å…¥å¤„ç†å™¨", SpecialIncomeProcessor),
            ("æ•°æ®åˆå¹¶å¤„ç†å™¨", DataMerger),
        ]

        results = {}
        success_count = 0

        for processor_name, processor_class in processor_classes:
            self.logger.info(f"\nâ–¶ å¼€å§‹æ‰§è¡Œ: {processor_name}")
            try:
                processor = processor_class(self.base_data_dir)
                success = processor.run()
                results[processor_name] = "æˆåŠŸ" if success else "å¤±è´¥"
                if success:
                    success_count += 1
                self.logger.info(f"âœ” {processor_name} æ‰§è¡Œå®Œæˆ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            except Exception as e:
                self.logger.error(f"âœ— {processor_name} æ‰§è¡Œå¼‚å¸¸: {str(e)}", exc_info=True)
                results[processor_name] = "å¼‚å¸¸"

        # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
        self.logger.info("\n" + "=" * 80)
        self.logger.info("æ•°æ®å¤„ç†ä»»åŠ¡æ‰§è¡Œæ€»ç»“")
        self.logger.info("=" * 80)

        for processor_name, result in results.items():
            status_icon = "âœ…" if result == "æˆåŠŸ" else "âŒ"
            self.logger.info(f"{status_icon} {processor_name}: {result}")

        self.logger.info(f"\næ€»è®¡: {success_count}/{len(processor_classes)} ä¸ªä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")

        if success_count == len(processor_classes):
            self.logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†ä»»åŠ¡å‡æ‰§è¡ŒæˆåŠŸ!")
        else:
            self.logger.warning(f"âš ï¸  {len(processor_classes) - success_count} ä¸ªä»»åŠ¡æ‰§è¡Œå¤±è´¥æˆ–å¼‚å¸¸")

        self.logger.info("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    try:
        base_data_dir = r"E:\powerbi_data\çœ‹æ¿æ•°æ®"
        main_processor = MainDataProcessor(base_data_dir)
        main_processor.run_all_processors()
    except Exception as e:
        print(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()