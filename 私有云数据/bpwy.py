import pandas as pd
import numpy as np
import os
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Optional, Dict, Tuple
from logging.handlers import RotatingFileHandler
import datetime
import warnings
warnings.filterwarnings('ignore', category=FutureWarning, message='.*DataFrame concatenation with empty or all-NA entries.*')

class InsuranceWarrantyIntegrator:
    """
    ä¿é™©ä¸ä¿ä¿®æ•°æ®æ•´åˆå¤„ç†å™¨ï¼šç»Ÿä¸€å¤„ç†ä¿èµ”æ— å¿§ã€å…¨ä¿æ— å¿§ã€æ–°è½¦ä¿é™©å°è´¦æ•°æ®
    æ ¸å¿ƒåŠŸèƒ½ï¼šå¤šçº¿ç¨‹è¯»å–æ–‡ä»¶ã€æ•°æ®æ¸…æ´—æ ‡å‡†åŒ–ã€æ•°æ®åˆå¹¶ã€è¿è¥è½¦è¿‡æ»¤ã€ç»“æœé›†ä¸­è¾“å‡º
    """

    def __init__(self,
                 base_output_dir: str = r"E:\powerbi_data\çœ‹æ¿æ•°æ®\dashboard",
                 logger_output_dir: str = r"E:\powerbi_data\ä»£ç æ‰§è¡Œ\data\ç§æœ‰äº‘æ—¥å¿—",
                 supplement_car_path: str = r"C:\Users\åˆ˜æ´‹\Documents\WXWork\1688858189749305\WeDrive\æˆéƒ½æ°¸ä¹ç››ä¸–\ç»´æŠ¤æ–‡ä»¶\çœ‹æ¿éƒ¨åˆ†æ•°æ®æº\å„å…¬å¸é“¶è¡Œé¢åº¦.xlsx",
                 insurance_csv_path: str = r"C:\Users\åˆ˜æ´‹\Documents\WXWork\1688858189749305\WeDrive\æˆéƒ½æ°¸ä¹ç››ä¸–\ç»´æŠ¤æ–‡ä»¶\æ–°è½¦ä¿é™©å°è´¦-2025.csv"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨ï¼Œé›†ä¸­ç®¡ç†è·¯å¾„ã€é…ç½®å‚æ•°ï¼Œå®ç°è¾“å…¥è¾“å‡ºé›†ä¸­åŒ–

        :param base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼ˆæ‰€æœ‰ç»“æœæ–‡ä»¶é›†ä¸­å­˜æ”¾ï¼‰
        :param supplement_car_path: è¡¥å……è½¦ç³»æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä¾èµ–æ–‡ä»¶ï¼‰
        :param insurance_csv_path: æ–°è½¦ä¿é™©CSVå°è´¦è·¯å¾„ï¼ˆä¾èµ–æ–‡ä»¶ï¼‰
        """
        # -------------------------- 1. åˆå§‹åŒ–æ—¥å¿—é…ç½® --------------------------
        self.logger = self._init_logger(logger_output_dir)
        self._supplement_car_read = False  # æ ‡è®°è¡¥å……è½¦ç³»æ•°æ®æ˜¯å¦å·²è¯»å–ï¼ˆé¿å…é‡å¤æ—¥å¿—ï¼‰

        # -------------------------- 2. è·¯å¾„é›†ä¸­é…ç½®ï¼ˆè¾“å…¥è¾“å‡ºç»Ÿä¸€ç®¡ç†ï¼‰--------------------------
        self.input_paths = {
            "bpwy": r"E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\è¡ç”Ÿäº§å“",  # ä¿èµ”æ— å¿§æ•°æ®ç›®å½•
            "qbwy": r"E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\å…¨ä¿æ— å¿§",  # å…¨ä¿æ— å¿§æ•°æ®ç›®å½•
            "insurance": r"E:\powerbi_data\çœ‹æ¿æ•°æ®\ç§æœ‰äº‘æ–‡ä»¶æœ¬åœ°\æ–°è½¦ä¿é™©å°è´¦"  # æ–°è½¦ä¿é™©æ•°æ®ç›®å½•
        }
        self.supplement_car_path = supplement_car_path
        self.insurance_csv_path = insurance_csv_path
        self.output_dir = base_output_dir
        self.output_files = {
            "bpwy": os.path.join(self.output_dir, "ä¿èµ”æ— å¿§.csv"),
            "qbwy": os.path.join(self.output_dir, "å…¨èµ”æ— å¿§.csv"),
            "insurance": os.path.join(self.output_dir, "æ–°è½¦ä¿é™©å°è´¦.csv")
        }

        # -------------------------- 3. æ ¸å¿ƒé…ç½®å‚æ•° --------------------------
        self.sheet_names = {
            "bpwy": "ç™»è®°è¡¨",
            "qbwy": "å…¨ä¿æ— å¿§ç™»è®°è¡¨",
            "insurance": "æ–°è½¦å°è´¦æ˜ç»†"
        }
        self.required_cols = {
            "bpwy": [
                'è½¦æ¶å·', 'è½¦ç³»', 'é”€å”®æ—¥æœŸ', 'å¼€ç¥¨æ—¥æœŸ', 'å®¢æˆ·å§“å', 'æ‰‹æœºå·ç ',
                'ä¿èµ”æ— å¿§é‡‘é¢', 'åŒä¿æ— å¿§é‡‘é¢', 'ç»ˆèº«ä¿å…»é‡‘é¢', 'é”€å”®é¡¾é—®', 'æ‰€å±é—¨åº—', 'å¤‡æ³¨', 'æ—¥æœŸ'
            ],
            "qbwy": [
                'å®¢æˆ·å§“å', 'æ‰‹æœºå·ç ', 'èº«ä»½è¯å·', 'è½¦æ¶å·', 'å‘åŠ¨æœºå·', 'è½¦ç‰Œå·', 'è½¦ç³»',
                'æ–°è½¦å¼€ç¥¨ä»·æ ¼', 'è½¦æŸé™©ä¿é¢', 'è½¦è¾†ç±»å‹', 'è½¦ç³»ç½‘ç»œ', 'é”€å”®æ—¥æœŸ', 'å…¨ä¿æ— å¿§ç‰ˆæœ¬',
                'å…¨ä¿æ— å¿§é‡‘é¢', 'èµ·ä¿æ—¥æœŸ', 'ç»ˆæ­¢æ—¥æœŸ', 'é”€å”®é¡¾é—®', 'æ‰€å±é—¨åº—', 'æŠ•ä¿è´¹ç”¨', 'from'
            ],
            "insurance": [
                'æœˆä»½', 'ç­¾å•æ—¥æœŸ', 'åˆ°æœŸæ—¥æœŸ', 'ä¿é™©å…¬å¸', 'æ•°æ®å½’å±é—¨åº—', 'å½’å±å…¬å¸',
                'è½¦å‹', 'è½¦ç‰Œå·', 'è½¦æ¶å·', 'è¢«ä¿é™©äºº', 'äº¤å¼ºé™©ä¿è´¹', 'é”€å”®é¡¾é—®', 'æ˜¯å¦ä¸ºä¿èµ”æ— å¿§å®¢æˆ·'
            ]
        }
        self.qbwy_final_cols = [
            'å®¢æˆ·å§“å', 'æ‰‹æœºå·ç ', 'è½¦æ¶å·', 'è½¦ç³»', 'é”€å”®æ—¥æœŸ', 'å…¨ä¿æ— å¿§ç‰ˆæœ¬',
            'å…¨ä¿æ— å¿§é‡‘é¢', 'æ‰€å±é—¨åº—', 'é”€å”®é¡¾é—®'
        ]
        self.business_rules = {
            "company_mapping": {"æ–‡æ™¯åˆæ²»": "ä¸Šå…ƒç››ä¸–", "ç‹æœç½‘-ç›´æ’­åŸºåœ°":"ç›´æ’­åŸºåœ°"},
            "exclude_operating_fee": [1000, 1130, 1800],
            "exclude_operating_company": "é¼å’Œ",
            "max_workers": 5
        }

        # -------------------------- 4. åˆå§‹åŒ–ç¯å¢ƒæ£€æŸ¥ --------------------------
        self._init_environment_check()

    def _init_logger(self, log_dir: str) -> logging.Logger:
        """åˆå§‹åŒ–æ—¥å¿—é…ç½®ï¼šç²¾ç®€è¾“å‡ºï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯"""
        log_dir = os.path.join(log_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)

        log_filename = f"insurance_integration_{datetime.datetime.now().strftime('%Y%m%d')}.log"
        log_filepath = os.path.join(log_dir, log_filename)

        # ç®€åŒ–æ—¥å¿—æ ¼å¼ï¼ˆå»æ‰å†—ä½™å­—æ®µï¼‰
        log_format = "%(asctime)s - %(levelname)s - %(message)s"
        date_format = "%Y-%m-%d %H:%M:%S"

        logger = logging.getLogger("InsuranceWarrantyIntegrator")
        logger.setLevel(logging.INFO)
        logger.propagate = False

        # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆä»…è¾“å‡ºå…³é”®ä¿¡æ¯ï¼‰
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(logging.Formatter(log_format, date_format))

        # æ–‡ä»¶å¤„ç†å™¨ï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼ŒæŒ‰å¤§å°è½®è½¬ï¼‰
        file_handler = RotatingFileHandler(
            log_filepath,
            maxBytes=10 * 1024 * 1024,
            backupCount=5,
            encoding="utf-8"
        )
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter(log_format, date_format))

        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

        return logger

    def _init_environment_check(self) -> None:
        """åˆå§‹åŒ–ç¯å¢ƒæ£€æŸ¥ï¼šä»…è¾“å‡ºå…³é”®ç»“æœæ—¥å¿—"""
        self.logger.info("åˆå§‹åŒ–ç¯å¢ƒæ£€æŸ¥...")

        # æ£€æŸ¥è¾“å…¥ç›®å½•å’Œä¾èµ–æ–‡ä»¶ï¼ˆé”™è¯¯æ‰è¾“å‡ºï¼ŒæˆåŠŸé™é»˜ï¼‰
        try:
            for key, path in self.input_paths.items():
                if not os.path.exists(path):
                    raise FileNotFoundError(f"{key}æ•°æ®è¾“å…¥ç›®å½•ï¼š{path}")

            for file_path in [self.supplement_car_path, self.insurance_csv_path]:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"ä¾èµ–æ–‡ä»¶ï¼š{file_path}")

            os.makedirs(self.output_dir, exist_ok=True)
            self.logger.info("åˆå§‹åŒ–å®Œæˆ âœ…")
        except FileNotFoundError as e:
            self.logger.error(f"åˆå§‹åŒ–å¤±è´¥ âŒï¼š{str(e)}")
            raise

    # -------------------------- é€šç”¨å·¥å…·æ–¹æ³• --------------------------
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–åˆ—åï¼ˆæ— æ—¥å¿—ï¼‰"""
        df.columns = df.columns.str.strip().str.lower().str.replace(r'\s+', '_', regex=True)
        return df

    def _make_unique_columns(self, column_names: List[str]) -> List[str]:
        """åˆ—åå»é‡ï¼ˆæ— æ—¥å¿—ï¼‰"""
        unique_names = []
        name_count: Dict[str, int] = {}
        for name in column_names:
            if name in name_count:
                name_count[name] += 1
                unique_names.append(f"{name}_{name_count[name]}")
            else:
                name_count[name] = 0
                unique_names.append(name)
        return unique_names

    def _read_supplement_car(self) -> pd.DataFrame:
        """è¯»å–è¡¥å……è½¦ç³»æ•°æ®ï¼ˆä»…é¦–æ¬¡è¯»å–è¾“å‡ºæ—¥å¿—ï¼‰"""
        if self._supplement_car_read:
            df_car = pd.read_excel(self.supplement_car_path, sheet_name="è¡¥å……è½¦ç³»")
            return df_car[["è½¦ç³»", "æœåŠ¡ç½‘ç»œ"]]

        try:
            df_car = pd.read_excel(self.supplement_car_path, sheet_name="è¡¥å……è½¦ç³»")
            self.logger.info(f"è¡¥å……è½¦ç³»æ•°æ®åŠ è½½å®Œæˆï¼š{len(df_car)}è¡Œ")
            self._supplement_car_read = True
            return df_car[["è½¦ç³»", "æœåŠ¡ç½‘ç»œ"]]
        except Exception as e:
            self.logger.error(f"è¡¥å……è½¦ç³»æ•°æ®è¯»å–å¤±è´¥ âŒï¼š{str(e)}", exc_info=True)
            raise RuntimeError(f"è¡¥å……è½¦ç³»æ•°æ®è¯»å–å¤±è´¥ï¼š{str(e)}") from e

    def _read_excel_multi_thread(self, module_key: str) -> List[pd.DataFrame]:
        """é€šç”¨Excelå¤šçº¿ç¨‹è¯»å–ï¼ˆä»…è¾“å‡ºæ±‡æ€»æ—¥å¿—ï¼Œå»æ‰å•ä¸ªæ–‡ä»¶æ—¥å¿—ï¼‰"""
        module_name = {"bpwy": "ä¿èµ”æ— å¿§", "qbwy": "å…¨ä¿æ— å¿§"}.get(module_key, module_key)
        directory = self.input_paths[module_key]
        sheet_name = self.sheet_names[module_key]
        required_cols = self.required_cols[module_key]
        max_workers = self.business_rules["max_workers"]

        dfs: List[pd.DataFrame] = []
        failed_files = []  # ä»…è®°å½•å¤±è´¥æ–‡ä»¶ï¼Œç»Ÿä¸€è¾“å‡º

        def _read_single_file(file_path: str) -> Optional[pd.DataFrame]:
            """è¯»å–å•ä¸ªæ–‡ä»¶ï¼ˆæ— æ—¥å¿—ï¼Œä»…è®°å½•å¤±è´¥ï¼‰"""
            filename = os.path.basename(file_path)
            try:
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=0, dtype=str)
                df["from"] = filename.split('.')[0]
                df = self._standardize_columns(df)
                for col in required_cols:
                    if col not in df.columns:
                        df[col] = None
                return df[required_cols].copy()
            except Exception:
                failed_files.append(filename)
                return None

        # è·å–Excelæ–‡ä»¶
        excel_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.xlsx')]
        if not excel_files:
            self.logger.warning(f"{module_name}ï¼šæ— å¾…å¤„ç†Excelæ–‡ä»¶ âš ï¸")
            return []

        # å¤šçº¿ç¨‹è¯»å–
        self.logger.info(f"{module_name}ï¼šå¼€å§‹è¯»å–{len(excel_files)}ä¸ªæ–‡ä»¶...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_read_single_file, f): f for f in excel_files}
            for future in as_completed(futures):
                res = future.result()
                if res is not None:
                    dfs.append(res)

        # è¾“å‡ºæ±‡æ€»æ—¥å¿—
        total_success = len(dfs)
        total_failed = len(failed_files)
        self.logger.info(f"{module_name}ï¼šè¯»å–å®Œæˆ â†’ æˆåŠŸ{total_success}ä¸ªï¼Œå¤±è´¥{total_failed}ä¸ª")
        if failed_files:
            self.logger.warning(
                f"{module_name}ï¼šå¤±è´¥æ–‡ä»¶ï¼š{','.join(failed_files[:5])}{'...' if len(failed_files) > 5 else ''}")

        return dfs

    # -------------------------- ä¿èµ”æ— å¿§æ•°æ®å¤„ç† --------------------------
    def _process_bpwy_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ä¿èµ”æ— å¿§æ•°æ®ï¼ˆæ— æ—¥å¿—ï¼‰"""
        df['å¼€ç¥¨æ—¥æœŸ'] = pd.to_datetime(df['å¼€ç¥¨æ—¥æœŸ'], format='mixed', errors='coerce')
        df['é”€å”®æ—¥æœŸ'] = pd.to_datetime(df['é”€å”®æ—¥æœŸ'], format='mixed', errors='coerce')
        df['å¼€ç¥¨æ—¥æœŸ'] = np.where(df['å¼€ç¥¨æ—¥æœŸ'] <= df['é”€å”®æ—¥æœŸ'], df['é”€å”®æ—¥æœŸ'], df['å¼€ç¥¨æ—¥æœŸ'])
        df['æ—¥æœŸ'] = df['å¼€ç¥¨æ—¥æœŸ'].fillna(df['é”€å”®æ—¥æœŸ'])
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], format='mixed', errors='coerce').dt.date
        return df[self.required_cols["bpwy"]]

    def process_bpwy(self) -> pd.DataFrame:
        """ä¿èµ”æ— å¿§å®Œæ•´æµç¨‹ï¼ˆä»…è¾“å‡ºå…³é”®èŠ‚ç‚¹æ—¥å¿—ï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹å¤„ç†ã€ä¿èµ”æ— å¿§ã€‘æ•°æ®")

        dfs = self._read_excel_multi_thread(module_key="bpwy")
        if not dfs:
            self.logger.error("ä¿èµ”æ— å¿§ï¼šæ— æœ‰æ•ˆæ•°æ® âŒ")
            raise ValueError("æœªè¯»å–åˆ°ä¿èµ”æ— å¿§æœ‰æ•ˆæ•°æ®")

        df_combined = pd.concat(dfs, axis=0, join='outer', ignore_index=True)
        df_processed = self._process_bpwy_data(df_combined)

        self.logger.info(f"ä¿èµ”æ— å¿§ï¼šå¤„ç†å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_processed)}è¡Œ âœ…")
        return df_processed

    # -------------------------- å…¨ä¿æ— å¿§æ•°æ®å¤„ç† --------------------------
    def _process_qbwy_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†å…¨ä¿æ— å¿§æ•°æ®ï¼ˆæ— æ—¥å¿—ï¼‰"""
        df['é”€å”®æ—¥æœŸ'] = pd.to_datetime(df['é”€å”®æ—¥æœŸ'], format='mixed', errors='coerce').dt.date
        df_car = self._read_supplement_car()
        df = pd.merge(df, df_car, how='left', on='è½¦ç³»')
        df['æ‰€å±é—¨åº—'] = np.where(
            df['æ‰€å±é—¨åº—'] == 'ç›´æ’­åŸºåœ°',
            df['æœåŠ¡ç½‘ç»œ'] + '-' + df['æ‰€å±é—¨åº—'],
            df['æ‰€å±é—¨åº—']
        )
        return df[self.required_cols["qbwy"]]

    def process_qbwy(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """å…¨ä¿æ— å¿§å®Œæ•´æµç¨‹ï¼ˆä»…è¾“å‡ºå…³é”®èŠ‚ç‚¹æ—¥å¿—ï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹å¤„ç†ã€å…¨ä¿æ— å¿§ã€‘æ•°æ®")

        dfs = self._read_excel_multi_thread(module_key="qbwy")
        if not dfs:
            self.logger.error("å…¨ä¿æ— å¿§ï¼šæ— æœ‰æ•ˆæ•°æ® âŒ")
            raise ValueError("æœªè¯»å–åˆ°å…¨ä¿æ— å¿§æœ‰æ•ˆæ•°æ®")

        df_combined = pd.concat(dfs, axis=0, join='outer', ignore_index=True)
        df_processed = self._process_qbwy_data(df_combined)

        # ç­›é€‰å¤„ç†
        df_qbwy1 = df_processed.drop_duplicates().query("æ‰€å±é—¨åº—.notnull()")
        df_qbwy2 = df_qbwy1[self.qbwy_final_cols].copy()
        df_qbwy2['æ—¥æœŸ'] = df_qbwy2['é”€å”®æ—¥æœŸ']

        self.logger.info(f"å…¨ä¿æ— å¿§ï¼šå¤„ç†å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_qbwy2)}è¡Œ âœ…")
        return df_qbwy1, df_qbwy2

    # -------------------------- ä¿èµ”+å…¨ä¿æ•°æ®åˆå¹¶ --------------------------
    def merge_warranty_data(self, df_bpwy: pd.DataFrame, df_qbwy2: pd.DataFrame) -> pd.DataFrame:
        """åˆå¹¶ä¿èµ”+å…¨ä¿æ•°æ®ï¼ˆä»…è¾“å‡ºåˆå¹¶ç»“æœï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹åˆå¹¶ã€ä¿èµ”+å…¨ä¿ã€‘æ•°æ®")

        df_wuyou = pd.concat([df_qbwy2, df_bpwy], axis=0, join='outer', ignore_index=True)
        df_car = self._read_supplement_car()
        df_wuyou = pd.merge(df_wuyou, df_car, how='left', on='è½¦ç³»')
        df_wuyou['æ‰€å±é—¨åº—'] = np.where(
            df_wuyou['æ‰€å±é—¨åº—'] == 'ç›´æ’­åŸºåœ°',
            df_wuyou['æœåŠ¡ç½‘ç»œ'] + '-' + df_wuyou['æ‰€å±é—¨åº—'],
            df_wuyou['æ‰€å±é—¨åº—']
        )

        # ä¸šåŠ¡è§„åˆ™åº”ç”¨
        df_wuyou['æ˜¯å¦ä¿èµ”'] = 'æ˜¯'
        df_wuyou['æ‰€å±é—¨åº—'] = df_wuyou['æ‰€å±é—¨åº—'].replace(self.business_rules["company_mapping"])
        df_wuyou['åŸå¸‚'] = np.where(df_wuyou['æ‰€å±é—¨åº—'].str.contains('è´µå·'), 'è´µå·', 'æˆéƒ½')
        df_wuyou = df_wuyou.drop_duplicates().dropna(subset='è½¦æ¶å·')

        self.logger.info(f"åˆå¹¶å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_wuyou)}è¡Œ âœ…")
        return df_wuyou

    # -------------------------- æ–°è½¦ä¿é™©æ•°æ®å¤„ç† --------------------------
    def _read_insurance_excel(self) -> pd.DataFrame:
        """è¯»å–æ–°è½¦ä¿é™©Excelï¼ˆä»…è¾“å‡ºæ±‡æ€»æ—¥å¿—ï¼‰"""
        directory = self.input_paths["insurance"]
        sheet_name = self.sheet_names["insurance"]
        max_workers = self.business_rules["max_workers"]

        dfs: List[pd.DataFrame] = []
        failed_files = []

        def _read_single_file(filename: str) -> Optional[pd.DataFrame]:
            """è¯»å–å•ä¸ªä¿é™©Excelï¼ˆæ— æ—¥å¿—ï¼Œä»…è®°å½•å¤±è´¥ï¼‰"""
            if 'æ–°è½¦' in filename and filename.endswith('.xlsx'):
                file_path = os.path.join(directory, filename)
                try:
                    with pd.ExcelFile(file_path) as xls:
                        if sheet_name not in xls.sheet_names:
                            return None
                        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)
                        df['From'] = filename.split('.')[0]
                        df.columns = df.columns.str.replace('\n', '')
                        return df
                except Exception:
                    failed_files.append(filename)
            return None

        filenames = os.listdir(directory)
        self.logger.info(f"ä¿é™©Excelï¼šå¼€å§‹è¯»å–{len(filenames)}ä¸ªæ–‡ä»¶ï¼ˆç­›é€‰å«'æ–°è½¦'çš„xlsxï¼‰...")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(_read_single_file, fn): fn for fn in filenames}
            for future in as_completed(futures):
                res = future.result()
                if res is not None:
                    dfs.append(res)

        df_combined = pd.concat(dfs, axis=0, ignore_index=True) if dfs else pd.DataFrame()
        self.logger.info(f"ä¿é™©Excelï¼šè¯»å–å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_combined)}è¡Œï¼Œå¤±è´¥{len(failed_files)}ä¸ªæ–‡ä»¶")
        return df_combined

    def _process_insurance_csv(self) -> pd.DataFrame:
        """å¤„ç†æ–°è½¦ä¿é™©CSVï¼ˆä»…è¾“å‡ºå…³é”®æ—¥å¿—ï¼‰"""
        try:
            self.logger.info(f"ä¿é™©CSVï¼šå¼€å§‹è¯»å–{os.path.basename(self.insurance_csv_path)}...")
            df_cyy = pd.read_csv(self.insurance_csv_path)

            # æ•°æ®å¤„ç†ï¼ˆæ— ä¸­é—´æ—¥å¿—ï¼‰
            df_cyy = df_cyy[
                ['å‡ºå•æ—¥æœŸ', 'ä¿é™©å…¬å¸ç®€ç§°', 'æ‰€å±é—¨åº—', 'è½¦ç³»', 'è½¦æ¶å·', 'äº¤å¼ºé™©ä¿è´¹', 'ä¸šåŠ¡äººå‘˜', 'ä¿è´¹æ€»é¢']]
            df_cyy.rename(columns={
                'å‡ºå•æ—¥æœŸ': 'ç­¾å•æ—¥æœŸ', 'ä¿é™©å…¬å¸ç®€ç§°': 'ä¿é™©å…¬å¸', 'è½¦ç³»': 'è½¦å‹',
                'æ‰€å±é—¨åº—': 'å½’å±å…¬å¸', 'ä¸šåŠ¡äººå‘˜': 'é”€å”®é¡¾é—®'
            }, inplace=True)

            df_car = self._read_supplement_car()
            df_cyy = pd.merge(df_cyy, df_car, how='left', left_on='è½¦å‹', right_on='è½¦ç³»')
            df_cyy['å½’å±å…¬å¸'] = np.where(
                df_cyy['å½’å±å…¬å¸'] == 'ç›´æ’­åŸºåœ°',
                df_cyy['æœåŠ¡ç½‘ç»œ'] + '-' + df_cyy['å½’å±å…¬å¸'],
                df_cyy['å½’å±å…¬å¸']
            )

            self.logger.info(f"ä¿é™©CSVï¼šå¤„ç†å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_cyy)}è¡Œ âœ…")
            return df_cyy
        except Exception as e:
            self.logger.error(f"ä¿é™©CSVï¼šå¤„ç†å¤±è´¥ âŒï¼š{str(e)}", exc_info=True)
            raise RuntimeError(f"ä¿é™©CSVå¤„ç†å¤±è´¥ï¼š{str(e)}") from e

    def process_insurance(self) -> pd.DataFrame:
        """æ–°è½¦ä¿é™©å®Œæ•´æµç¨‹ï¼ˆä»…è¾“å‡ºå…³é”®èŠ‚ç‚¹æ—¥å¿—ï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹å¤„ç†ã€æ–°è½¦ä¿é™©ã€‘æ•°æ®")

        # è¯»å–Excelå’ŒCSV
        df_excel = self._read_insurance_excel()
        df_csv = self._process_insurance_csv()

        # åˆå¹¶å¤„ç†
        all_insurance_dfs = [df_excel] if not df_excel.empty else []
        all_insurance_dfs.append(df_csv)

        df_combined = pd.concat(all_insurance_dfs, axis=0, ignore_index=True)
        df_combined.columns = self._make_unique_columns(df_combined.columns)
        df_csv.columns = self._make_unique_columns(df_csv.columns)
        df_combined_all = pd.concat([df_combined, df_csv], axis=0, join='outer', ignore_index=True)

        # æ•°æ®æ¸…æ´—
        df_combined_all['å½’å±å…¬å¸'] = df_combined_all['å½’å±å…¬å¸'].replace(self.business_rules["company_mapping"])
        df_combined_all = df_combined_all.dropna(subset=['ä¿é™©å…¬å¸'])

        # ç­›é€‰å¿…å¡«åˆ—
        exist_cols = [col for col in self.required_cols["insurance"] if col in df_combined_all.columns]
        df_filtered = df_combined_all[exist_cols].copy()
        df_filtered['æ—¥æœŸ'] = pd.to_datetime(df_filtered['ç­¾å•æ—¥æœŸ'], errors='coerce').dt.date
        df_filtered = df_filtered.sort_values(by='æ—¥æœŸ', ascending=False).drop_duplicates(subset='è½¦æ¶å·', keep='first')

        self.logger.info(f"æ–°è½¦ä¿é™©ï¼šå¤„ç†å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_filtered)}è¡Œ âœ…")
        return df_filtered

    # -------------------------- ä¿é™©ä¸ä¿ä¿®æ•°æ®åˆå¹¶+è¿è¥è½¦è¿‡æ»¤ --------------------------
    def merge_insurance_with_warranty(self, df_insurance: pd.DataFrame, df_wuyou: pd.DataFrame) -> pd.DataFrame:
        """åˆå¹¶+è¿‡æ»¤è¿è¥è½¦ï¼ˆä»…è¾“å‡ºå…³é”®ç»“æœï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹åˆå¹¶ã€ä¿é™©+ä¿èµ”ã€‘æ•°æ®å¹¶è¿‡æ»¤è¿è¥è½¦")

        # åˆå¹¶ä¿èµ”æ ‡è®°
        df_merged = pd.merge(df_insurance, df_wuyou[['è½¦æ¶å·', 'æ˜¯å¦ä¿èµ”']], how='left', on='è½¦æ¶å·')
        df_merged['æ˜¯å¦ä¿èµ”'] = df_merged['æ˜¯å¦ä¿èµ”'].fillna('å¦')

        # ç­›é€‰è¿è¥è½¦
        df_exclude_company = df_merged[
            df_merged['ä¿é™©å…¬å¸'].str.contains(self.business_rules["exclude_operating_company"], na=False)]
        df_exclude_fee = df_merged[df_merged['äº¤å¼ºé™©ä¿è´¹'].isin(self.business_rules["exclude_operating_fee"])]
        df_excluded = pd.concat([df_exclude_company, df_exclude_fee], axis=0).drop_duplicates().query(
            "æ˜¯å¦ä¿èµ” == 'å¦'")

        # æœ‰æ•ˆæ•°æ®
        df_valid = df_merged[~df_merged['è½¦æ¶å·'].isin(df_excluded['è½¦æ¶å·'])].copy()
        df_valid['åŸå¸‚'] = np.where(df_valid['å½’å±å…¬å¸'].str.contains('è´µå·'), 'è´µå·', 'æˆéƒ½')
        df_valid = df_valid.drop_duplicates()

        self.logger.info(f"åˆå¹¶è¿‡æ»¤å®Œæˆ â†’ æœ‰æ•ˆæ•°æ®{len(df_valid)}è¡Œï¼Œæ’é™¤è¿è¥è½¦{len(df_excluded)}è¡Œ âœ…")
        return df_valid

    # -------------------------- ç»“æœä¿å­˜ --------------------------
    def _save_results(self, df_wuyou: pd.DataFrame, df_qbwy1: pd.DataFrame, df_valid_insurance: pd.DataFrame) -> None:
        """ä¿å­˜ç»“æœï¼ˆä»…è¾“å‡ºä¿å­˜çŠ¶æ€ï¼‰"""
        self.logger.info("\n" + "-" * 50)
        self.logger.info("å¼€å§‹ä¿å­˜ç»“æœæ–‡ä»¶")

        # ä¿å­˜ä¸‰ä¸ªæ–‡ä»¶ï¼ˆæ— å•ä¸ªæ–‡ä»¶æ—¥å¿—ï¼Œç»Ÿä¸€è¾“å‡ºç»“æœï¼‰
        try:
            df_wuyou.to_csv(self.output_files["bpwy"], index=False, encoding='utf-8-sig')
            df_qbwy1.to_csv(self.output_files["qbwy"], index=False, encoding='utf-8-sig')
            df_valid_insurance.to_csv(self.output_files["insurance"], index=False, encoding='utf-8-sig')
            self.logger.info("ç»“æœæ–‡ä»¶ä¿å­˜å®Œæˆ âœ…")
            self.logger.info(f"  - ä¿èµ”æ— å¿§ï¼š{os.path.basename(self.output_files['bpwy'])}ï¼ˆ{len(df_wuyou)}è¡Œï¼‰")
            self.logger.info(f"  - å…¨èµ”æ— å¿§ï¼š{os.path.basename(self.output_files['qbwy'])}ï¼ˆ{len(df_qbwy1)}è¡Œï¼‰")
            self.logger.info(f"  - æ–°è½¦ä¿é™©å°è´¦ï¼š{os.path.basename(self.output_files['insurance'])}ï¼ˆ{len(df_valid_insurance)}è¡Œï¼‰")
        except Exception as e:
            self.logger.error(f"ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥ âŒï¼š{str(e)}", exc_info=True)
            raise

    # -------------------------- ä¸»æ‰§è¡Œå…¥å£ --------------------------
    def run(self) -> None:
        """å®Œæ•´æµç¨‹æ‰§è¡Œå…¥å£ï¼ˆç²¾ç®€æµç¨‹æ—¥å¿—ï¼‰"""
        self.logger.info("=" * 60)
        self.logger.info("ã€ä¿é™©ä¸ä¿ä¿®æ•°æ®æ•´åˆå¤„ç†å™¨ã€‘å¯åŠ¨")
        self.logger.info("=" * 60)

        try:
            # 1. å¤„ç†å…¨ä¿æ— å¿§
            df_qbwy1, df_qbwy2 = self.process_qbwy()

            # 2. å¤„ç†ä¿èµ”æ— å¿§
            df_bpwy = self.process_bpwy()

            # 3. åˆå¹¶ä¿èµ”+å…¨ä¿
            df_wuyou = self.merge_warranty_data(df_bpwy, df_qbwy2)

            # 4. å¤„ç†æ–°è½¦ä¿é™©
            df_insurance = self.process_insurance()

            # 5. åˆå¹¶+è¿‡æ»¤è¿è¥è½¦
            df_valid_insurance = self.merge_insurance_with_warranty(df_insurance, df_wuyou)

            # 6. ä¿å­˜ç»“æœ
            self._save_results(df_wuyou, df_qbwy1, df_valid_insurance)

            self.logger.info("\n" + "=" * 60)
            self.logger.info("ã€ä¿é™©ä¸ä¿ä¿®æ•°æ®æ•´åˆå¤„ç†å™¨ã€‘æ‰§è¡Œå®Œæˆï¼ğŸ‰")
            self.logger.info("=" * 60)
        except Exception as e:
            self.logger.error(f"\nã€æ‰§è¡Œé”™è¯¯ã€‘å¤„ç†æµç¨‹ä¸­æ–­ âŒï¼š{str(e)}", exc_info=True)
            raise


if __name__ == "__main__":
    processor = InsuranceWarrantyIntegrator()
    processor.run()